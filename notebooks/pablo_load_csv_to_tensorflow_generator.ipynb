{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "pablo-load_csv_to_tensorflow_generator",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label image classification with ImageDataGenerator"
      ],
      "metadata": {
        "id": "AylCVVQvOkIB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "source": [
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# tensorflow and keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, MaxPooling2D, Dropout, Input, Conv2D, Flatten\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "import datetime\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "metadata": {
        "id": "_9Sg9B4zHmCm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "DIRECTORY = \"/home/jupyter/\" # Path to the directory which contains CSVs and the folder 'images'\n",
        "CSV_NAME = \"dataset.csv\" # Name of the CSV we want to use\n",
        "NUM_MOVEMENT = 27 # Number of movements to classify\n",
        "NUM_GENRE = 10 # Number of genres to classify\n",
        "IMG_HEIGHT = IMG_WIDTH = 224 # Model's inputs shapes\n",
        "\n",
        "USER = \"pablo\" # Choose between 'common', 'pablo', 'quentin', 'gregoire', 'alex'\n",
        "MODEL_NAME = \"Custom\" # Set the name of the model \n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Load the CSV\n",
        "----------------------------------'''\n",
        "df = pd.read_csv(DIRECTORY + \"dataset.csv\")\n",
        "assert type(df) == type(pd.DataFrame()) # Check if we created a dataframe\n",
        "assert df.iloc[:, 1].nunique() ==  NUM_MOVEMENT # Check if we have the correct number of movements\n",
        "assert df.iloc[:, 2].nunique() ==  NUM_GENRE # Check if we have the correct number of genres\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Train, test, val split\n",
        "----------------------------------'''\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
        "assert type(df_train) == type(pd.DataFrame()) # Check if we created dataframes\n",
        "assert type(df_test) == type(pd.DataFrame())\n",
        "assert type(df_val) == type(pd.DataFrame())\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Setup outputs columns\n",
        "----------------------------------'''\n",
        "assert len(list(df.columns[1:])) == 2 # Check if we have two outputs columns\n",
        "columns=list(df.columns[1:])\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Train ImageDataGenerator\n",
        "----------------------------------'''\n",
        "train_datagen = ImageDataGenerator( # This generator is only used to train data because it has data augmentation and we do not want to augment data from the test or val set\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=(0.95, 0.95),\n",
        "    horizontal_flip=True,\n",
        "    dtype=tf.float32\n",
        "    )\n",
        "\n",
        "assert type(train_datagen) == type(ImageDataGenerator())\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=df_train, # Dataset used to get the path (column filename) and the linked outputs\n",
        "    directory=DIRECTORY + \"images/\", # Path to the images\n",
        "    x_col=\"filename\", # Column with the name of the images that the generator will get from the directory\n",
        "    y_col=columns, # Columns with the output of the images that the generator will get from the csv\n",
        "    batch_size=32,\n",
        "    seed=None,\n",
        "    shuffle=True,\n",
        "    class_mode=\"raw\", # numpy array of values in y_col columns\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH) # Resize the images to the input shape of the model\n",
        "    ) \n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Test and Val ImageDataGenerator\n",
        "----------------------------------'''\n",
        "test_val_datagen = ImageDataGenerator(  # We use a new generator without data augmentation\n",
        "    rescale=1./255\n",
        "    )\n",
        "\n",
        "val_generator = test_val_datagen.flow_from_dataframe(\n",
        "    dataframe=df_val, \n",
        "    directory=DIRECTORY + \"images/\",\n",
        "    x_col=\"filename\",\n",
        "    y_col=columns,\n",
        "    batch_size=32,\n",
        "    seed=None,\n",
        "    shuffle=False,\n",
        "    class_mode=\"raw\", \n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        "    )\n",
        "\n",
        "test_generator = test_val_datagen.flow_from_dataframe(\n",
        "    dataframe=df_test,\n",
        "    directory=DIRECTORY + \"images/\",\n",
        "    x_col=\"filename\",\n",
        "    batch_size=1,\n",
        "    seed=None,\n",
        "    shuffle=False,\n",
        "    class_mode=None, # No targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict()\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        "    )\n",
        "\n",
        "assert type(test_val_datagen) == type(ImageDataGenerator())\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "aPV2fKUWPuu9",
        "outputId": "a1cfda88-9673-4eee-f97b-b77ccc1ccb1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''----------------------------------\n",
        "Create model (Functional API)\n",
        "----------------------------------'''\n",
        "# Model tout pourri pour l'instant, transfert learning ensuite\n",
        "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "x = Conv2D(16, 3, padding='same', activation='relu'),\n",
        "x = MaxPooling2D(),\n",
        "x = Conv2D(32, 3, padding='same', activation='relu'),\n",
        "x = MaxPooling2D(),\n",
        "x = Conv2D(64, 3, padding='same', activation='relu'),\n",
        "x = MaxPooling2D(),\n",
        "x = Dropout(rate=0.3),\n",
        "x = Conv2D(128, 3, padding='same', activation='relu'),\n",
        "x = MaxPooling2D(),\n",
        "x = Dropout(rate=0.4),\n",
        "x = Flatten(),\n",
        "x = Dense(128, activation='relu'),\n",
        "x = Dropout(rate=0.5),\n",
        "\n",
        "output1 = Dense(NUM_MOVEMENT, activation = 'softmax')(x) # First output for movement\n",
        "output2 = Dense(NUM_GENRE, activation = 'softmax')(x) # Second outptu for genre\n",
        "\n",
        "model = Model(inputs, [output1,output2]) # Create the model with 2 outputs\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Compile model\n",
        "----------------------------------'''\n",
        "model.compile(optimizer='adamax', loss=['categorical_crossentropy', 'categorical_crossentropy'], metrics=['accuracy', 'accuracy'])\n",
        "# We have 2 losses and 2 accuracy, one for each output (not sure if we can set 2 metrics)\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Setup callbacks\n",
        "----------------------------------'''\n",
        "es = EarlyStopping(monitor='val_loss', patience=11, mode='min', restore_best_weights=True)\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=5, min_lr=1e-6)\n",
        "\n",
        "%load_ext tensorboard\n",
        "log_dir = f\"logs/{USER}/{MODEL_NAME}\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tsboard = TensorBoard(log_dir=log_dir)\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Fit model\n",
        "----------------------------------'''\n",
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size # see steps_per_epoch comment below\n",
        "STEP_SIZE_VAL = val_generator.n//val_generator.batch_size\n",
        "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=STEP_SIZE_TRAIN, # No 'batch_size' parameter for data coming from a generator or tf.Dataset, common choice is to use num_samples // batch_size\n",
        "    epochs=50,\n",
        "    validation_data=val_generator, # We use a generator as the validation_data\n",
        "    validation_steps=STEP_SIZE_VAL,\n",
        "    callbacks=[es, rlrp, tsboard],\n",
        "    verbose=1\n",
        "    )\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Evaluate\n",
        "----------------------------------'''\n",
        "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
        "pred = model.evaluate(\n",
        "    test_generator,\n",
        "    steps=STEP_SIZE_TEST,\n",
        "    verbose=1\n",
        "    )\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Predict\n",
        "----------------------------------'''\n",
        "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
        "pred = model.predict(\n",
        "    test_generator,\n",
        "    steps=STEP_SIZE_TEST,\n",
        "    verbose=1\n",
        "    )\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "TpsswJ_CfbTv"
      }
    }
  ]
}