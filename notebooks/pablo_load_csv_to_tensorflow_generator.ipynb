{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pablo-load_csv_to_tensorflow_generator",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AylCVVQvOkIB"
      },
      "source": [
        "# Multi-label image classification with ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Sg9B4zHmCm"
      },
      "source": [
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# tensorflow and keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import layers, models"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "aPV2fKUWPuu9",
        "outputId": "a1cfda88-9673-4eee-f97b-b77ccc1ccb1f"
      },
      "source": [
        "DIRECTORY = \"/ /\" # Path to the directory which contains CSVs and the folder 'images'\n",
        "CSV_NAME = \"dataset.csv/\" # Name of the CSV we want to use\n",
        "NUM_MOVEMENT = 27 # Number of movements to classify\n",
        "NUM_GENRE = 10 # Number of genres to classify\n",
        "IMG_HEIGHT = IMG_WIDTH = 224 # Model's inputs shapes\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Load the CSV\n",
        "----------------------------------'''\n",
        "df = pd.read_csv(DIRECTORY + \"dataset.csv/\")\n",
        "assert type(df) == type(pd.DataFrame()) # Check if we created a dataframe\n",
        "assert df.iloc[:, 1].nunique() ==  NUM_MOVEMENT # Check if we have the correct number of movements\n",
        "assert df.iloc[:, 2].nunique() ==  NUM_GENRE # Check if we have the correct number of genres\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Train, test, val split\n",
        "----------------------------------'''\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
        "assert type(df_train) == type(pd.DataFrame()) # Check if we created dataframes\n",
        "assert type(df_test) == type(pd.DataFrame())\n",
        "assert type(df_val) == type(pd.DataFrame())\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Setup outputs columns\n",
        "----------------------------------'''\n",
        "assert len(list(test_df.columns[1:])) == 2\n",
        "columns=list(test_df.columns[1:])\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Train ImageDataGenerator\n",
        "----------------------------------'''\n",
        "train_datagen = ImageDataGenerator( # This generator is only used for train data because it has data augmentation and we do not want to augment data from the test or val set\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=(0.95, 0.95),\n",
        "    horizontal_flip=True,\n",
        "    dtype=tf.float32)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=df_train, # Dataset used to get the path (column filename) and the linked outputs\n",
        "    directory=DIRECTORY + \"images/\", # Path to the images\n",
        "    x_col=\"filename\", # Column with the name of the images that the generator will get from the directory\n",
        "    y_col=columns, # Columns with the output of the images that the generator will get from the csv\n",
        "    batch_size=32,\n",
        "    seed=None,\n",
        "    shuffle=True,\n",
        "    class_mode=\"raw\", # numpy array of values in y_col columns\n",
        "    target_size=(IMG_HEIGTH, IMG_WIDTH)) # Resize the images to the input shape of the model\n",
        "\n",
        "assert type(train_datagen) == type(ImageDataGenerator())\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Test and Val ImageDataGenerator\n",
        "----------------------------------'''\n",
        "test_val_datagen = ImageDataGenerator(  # We use a new generator without data augmentation\n",
        "    rescale=1./255)\n",
        "\n",
        "val_generator = test_val_datagen.flow_from_dataframe(\n",
        "    dataframe=df_val, \n",
        "    directory=DIRECTORY + \"images/\",\n",
        "    x_col=\"filename\",\n",
        "    y_col=columns,\n",
        "    batch_size=32,\n",
        "    seed=None,\n",
        "    shuffle=False,\n",
        "    class_mode=\"raw\", \n",
        "    target_size=(IMG_HEIGTH, IMG_WIDTH))\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=df_test,\n",
        "    directory=DIRECTORY + \"images/\",\n",
        "    x_col=\"filename\",\n",
        "    batch_size=1,\n",
        "    seed=None,\n",
        "    shuffle=False,\n",
        "    class_mode=None, # No targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict()\n",
        "    target_size=(IMG_HEIGTH, IMG_WIDTH))\n",
        "\n",
        "\n",
        "assert type(test_val_datagen) == type(ImageDataGenerator())\n",
        "\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-1e145d93548d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mLoad\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ----------------------------------'''\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIRECTORY\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ /dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpsswJ_CfbTv"
      },
      "source": [
        "'''----------------------------------\n",
        "Create model (Functional API)\n",
        "----------------------------------'''\n",
        "# Model tout pourri pour l'instant, transfert learning ensuite\n",
        "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "x = layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "x = layers.MaxPooling2D(),\n",
        "x = layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "x = layers.MaxPooling2D(),\n",
        "x = layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "x = layers.MaxPooling2D(),\n",
        "x = layers.Dropout(rate=0.3),\n",
        "x = layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "x = layers.MaxPooling2D(),\n",
        "x = layers.Dropout(rate=0.4),\n",
        "x = layers.Flatten(),\n",
        "x = layers.Dense(128, activation='relu'),\n",
        "x = layers.Dropout(rate=0.5),\n",
        "\n",
        "output1 = Dense(NUM_MOVEMENT, activation = 'softmax')(x)\n",
        "output2 = Dense(NUM_GENRE, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(inputs, [output1,output2]) # Create the model with 2 outputs\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Compile model\n",
        "----------------------------------'''\n",
        "model.compile(optimizer='adamax', loss=['categorical_crossentropy', 'categorical_crossentropy'], metrics=['accuracy', 'accuracy'])\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Fit model\n",
        "----------------------------------'''\n",
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size # see steps_per_epoch comment below\n",
        "STEP_SIZE_VAL = val_generator.n//val_generator.batch_size\n",
        "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=STEP_SIZE_TRAIN, # No 'batch_size' parameter for data coming from a generator or tf.Dataset, common choixe is to use num_samples // batch_size\n",
        "    epochs=50,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=STEP_SIZE_VAL)\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Evaluate\n",
        "----------------------------------'''\n",
        "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
        "pred = model.evaluate(\n",
        "    test_generator,\n",
        "    steps=STEP_SIZE_TEST)\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Predict\n",
        "----------------------------------'''\n",
        "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
        "pred = model.predict(\n",
        "    test_generator,\n",
        "    steps=STEP_SIZE_TEST,\n",
        "    verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}