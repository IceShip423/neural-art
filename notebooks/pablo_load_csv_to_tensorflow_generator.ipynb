{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AylCVVQvOkIB"
   },
   "source": [
    "# Multi-label image classification with ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "_9Sg9B4zHmCm"
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Dropout, Input, Conv2D, Flatten, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Accuracy, CategoricalAccuracy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "aPV2fKUWPuu9",
    "outputId": "a1cfda88-9673-4eee-f97b-b77ccc1ccb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images : 2791\n",
      "Number of images in train set : 1785\n",
      "Number of images in val set : 447\n",
      "Number of images in test set : 559\n",
      "\n",
      "\n",
      "Found 1785 validated image filenames.\n",
      "Found 447 validated image filenames.\n",
      "Found 559 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAIN_PATH = \"/home/jupyter/\" # Path to the directory which contains CSVs and the folder 'dataset'\n",
    "IMAGES = \"dataset\"\n",
    "CSV_NAME = \"wikiart-movement-genre_True-class_3-merge_test1-n_1000_max.csv\" # Nwikiart-movement-genre_True-class_3-merge_test1-n_1000_max.csvme of the CSV we want to use\n",
    "NUM_MOVEMENT = 3 # Number of movements to classify\n",
    "NUM_GENRE = 10 # Number of genres to classify\n",
    "IMG_HEIGHT = IMG_WIDTH = 224 # Model's inputs shapes\n",
    "\n",
    "USER = \"pablo\" # Choose between 'common', 'pablo', 'quentin', 'gregoire', 'alex'\n",
    "MODEL_NAME = \"Custom v1\" # Set the name of the model \n",
    "\n",
    "'''----------------------------------\n",
    "Load the CSV\n",
    "----------------------------------'''\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "'''----------------------------------\n",
    "Load the CSV\n",
    "----------------------------------'''\n",
    "raw_df = pd.read_csv(MAIN_PATH + CSV_NAME)\n",
    "df = raw_df.iloc[:, 0:3]\n",
    "assert type(df) == type(pd.DataFrame()) # Check if we created a dataframe\n",
    "assert df.iloc[:, 1].nunique() ==  NUM_MOVEMENT # Check if we have the correct number of movements\n",
    "assert df.iloc[:, 2].nunique() ==  NUM_GENRE # Check if we have the correct number of genres\n",
    "\n",
    "print(f\"Number of images : {df.shape[0]}\")\n",
    "\n",
    "\n",
    "'''----------------------------------\n",
    "Train, test, val split\n",
    "----------------------------------'''\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
    "assert type(df_train) == type(pd.DataFrame()) # Check if we created dataframes\n",
    "assert type(df_test) == type(pd.DataFrame())\n",
    "assert type(df_val) == type(pd.DataFrame())\n",
    "\n",
    "print(f\"Number of images in train set : {df_train.shape[0]}\")\n",
    "print(f\"Number of images in val set : {df_val.shape[0]}\")\n",
    "print(f\"Number of images in test set : {df_test.shape[0]}\")\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "'''----------------------------------\n",
    "Setup outputs columns\n",
    "----------------------------------'''\n",
    "assert len(list(df.columns[1:])) == 2 # Check if we have two outputs columns\n",
    "columns=list(df.columns[1:])\n",
    "\n",
    "\n",
    "'''----------------------------------\n",
    "Train ImageDataGenerator\n",
    "----------------------------------'''\n",
    "train_datagen = ImageDataGenerator( # This generator is only used to train data because it has data augmentation and we do not want to augment data from the test or val set\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    zoom_range=(0.95, 0.95),\n",
    "    horizontal_flip=True,\n",
    "    dtype=tf.float32\n",
    "    )\n",
    "\n",
    "assert type(train_datagen) == type(ImageDataGenerator())\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train, # Dataset used to get the path (column filename) and the linked outputs\n",
    "    directory=MAIN_PATH + IMAGES, # Path to the images\n",
    "    x_col=\"file_name\", # Column with the name of the images that the generator will get from the directory\n",
    "    y_col=columns, # Columns with the output of the images that the generator will get from the csv\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=None,\n",
    "    shuffle=True,\n",
    "    class_mode=\"multi_output\", # numpy array of values in y_col columns\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Resize the images to the input shape of the model\n",
    "    data_format='channels_last'\n",
    "    ) \n",
    "\n",
    "\n",
    "'''----------------------------------\n",
    "Test and Val ImageDataGenerator\n",
    "----------------------------------'''\n",
    "test_val_datagen = ImageDataGenerator(  # We use a new generator without data augmentation\n",
    "    rescale=1./255\n",
    "    )\n",
    "\n",
    "val_generator = test_val_datagen.flow_from_dataframe(\n",
    "    dataframe=df_val, \n",
    "    directory=MAIN_PATH + IMAGES,\n",
    "    x_col=\"file_name\",\n",
    "    y_col=columns,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=None,\n",
    "    shuffle=False,\n",
    "    class_mode=\"multi_output\", \n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    data_format='channels_last'\n",
    "    )\n",
    "\n",
    "test_generator = test_val_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    directory=MAIN_PATH + IMAGES,\n",
    "    x_col=\"file_name\",\n",
    "    batch_size=1,\n",
    "    seed=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None, # No targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict()\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    data_format='channels_last'\n",
    "    )\n",
    "\n",
    "\n",
    "assert type(test_val_datagen) == type(ImageDataGenerator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Create model (Functional API)\n",
    "----------------------------------'''\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Model tout pourri pour l'instant, transfert learning ensuite\n",
    "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3), dtype=tf.float32)\n",
    "x = Conv2D(16, 3, padding='same', activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = Dropout(rate=0.3)(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.4)(x)\n",
    "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32)(x)\n",
    "\n",
    "movement_output = Dense(NUM_MOVEMENT, activation = 'softmax', name='movement_output')(x) # First output for movement\n",
    "genre_output = Dense(NUM_GENRE, activation = 'softmax', name='genre_output')(x) # Second outptu for genre\n",
    "\n",
    "model = Model(inputs, [movement_output,genre_output]) # Create the model with 2 outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 3), (None, 10)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Compile model\n",
    "----------------------------------'''\n",
    "model.compile(\n",
    "    optimizer='adamax', \n",
    "    loss={'movement_output': CategoricalCrossentropy(), \n",
    "          'genre_output': CategoricalCrossentropy()},\n",
    "    metrics={\n",
    "        \"movement_output\": [\n",
    "            Accuracy(),\n",
    "            ],\n",
    "        \"genre_output\": [\n",
    "            Accuracy(),\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "# We have 2 losses one for each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "'''----------------------------------\n",
    "Setup callbacks and tensorboard\n",
    "----------------------------------'''\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=11, mode='min', restore_best_weights=True)\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=5, min_lr=1e-6)\n",
    "\n",
    "%load_ext tensorboard\n",
    "log_dir = f\"logs/{USER}/{MODEL_NAME}\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tsboard = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 224, 224, 3)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_generator))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cubism', 'cubism', 'cubism', 'mannerism_late_renaissance',\n",
       "       'ukiyo_e', 'ukiyo_e', 'mannerism_late_renaissance',\n",
       "       'mannerism_late_renaissance', 'ukiyo_e',\n",
       "       'mannerism_late_renaissance', 'ukiyo_e', 'cubism',\n",
       "       'mannerism_late_renaissance', 'ukiyo_e', 'cubism', 'ukiyo_e',\n",
       "       'cubism', 'cubism', 'cubism', 'ukiyo_e',\n",
       "       'mannerism_late_renaissance', 'ukiyo_e', 'cubism', 'cubism',\n",
       "       'cubism', 'cubism', 'mannerism_late_renaissance', 'ukiyo_e',\n",
       "       'ukiyo_e', 'ukiyo_e', 'ukiyo_e', 'ukiyo_e'], dtype='<U26')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_generator))[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "TpsswJ_CfbTv"
   },
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Fit model\n",
    "----------------------------------'''\n",
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size # see steps_per_epoch comment below\n",
    "STEP_SIZE_VAL = val_generator.n//val_generator.batch_size\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
    "\n",
    "\n",
    "\n",
    "# We now have the model that has 2 output layers but our train_generator and valid_generator outputs a single array for the target labels, to handle this we need to write a python generator\n",
    "# function that takes train_generator or valid_generator as input and yields a tuple containing the images and a list containing 2(No. of outputs) arrays for the target labels.\n",
    "def generator_wrapper(generator):\n",
    "    for batch_x,batch_y in generator:\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "        yield (batch_x,[to_categorical(batch_y[:,i]) for i in range(2)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object generator_wrapper at 0x7fd122e45350>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-9ffd3b54ef63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_VAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrlrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-152-d055a9305b19>\u001b[0m in \u001b[0;36mgenerator_wrapper\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#         import ipdb; ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-152-d055a9305b19>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#         import ipdb; ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# The model does not see every image at each epoch !\n",
    "\n",
    "history = model.fit(\n",
    "    generator_wrapper(train_generator),\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN, # No 'batch_size' parameter for data coming from a generator or tf.Dataset, common choice is to use num_samples // batch_size\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=generator_wrapper(val_generator), # We use a generator as the validation_data\n",
    "    validation_steps=STEP_SIZE_VAL,\n",
    "    callbacks=[es, rlrp, tsboard],\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Evaluate\n",
    "----------------------------------'''\n",
    "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
    "pred = model.evaluate(\n",
    "    test_generator,\n",
    "    steps=STEP_SIZE_TEST,\n",
    "    verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Predict\n",
    "----------------------------------'''\n",
    "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
    "pred = model.predict(\n",
    "    test_generator,\n",
    "    steps=STEP_SIZE_TEST,\n",
    "    verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pablo-load_csv_to_tensorflow_generator",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
