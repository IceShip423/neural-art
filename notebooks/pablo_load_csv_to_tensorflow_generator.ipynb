{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "pablo-load_csv_to_tensorflow_generator",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label image classification with ImageDataGenerator"
      ],
      "metadata": {
        "id": "AylCVVQvOkIB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "source": [
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# tensorflow and keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D, Dropout, Input, Conv2D, Flatten, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "import datetime\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "metadata": {
        "id": "_9Sg9B4zHmCm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "MAIN_PATH = \"/home/jupyter/\" # Path to the directory which contains CSVs and the folder 'dataset'\n",
        "IMAGES = \"dataset\"\n",
        "CSV_NAME = \"wikiart-movement-genre_True-class_3-merge_test1-n_1000_max.csv\" # Nwikiart-movement-genre_True-class_3-merge_test1-n_1000_max.csvme of the CSV we want to use\n",
        "NUM_MOVEMENT = 3 # Number of movements to classify\n",
        "NUM_GENRE = 10 # Number of genres to classify\n",
        "IMG_HEIGHT = IMG_WIDTH = 224 # Model's inputs shapes\n",
        "\n",
        "USER = \"pablo\" # Choose between 'common', 'pablo', 'quentin', 'gregoire', 'alex'\n",
        "MODEL_NAME = \"Custom v1\" # Set the name of the model \n",
        "\n",
        "'''----------------------------------\n",
        "Load the CSV\n",
        "----------------------------------'''\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Load the CSV\n",
        "----------------------------------'''\n",
        "raw_df = pd.read_csv(DIRECTORY + CSV_NAME)\n",
        "df = raw_df.iloc[:, 0:3]\n",
        "assert type(df) == type(pd.DataFrame()) # Check if we created a dataframe\n",
        "assert df.iloc[:, 1].nunique() ==  NUM_MOVEMENT # Check if we have the correct number of movements\n",
        "assert df.iloc[:, 2].nunique() ==  NUM_GENRE # Check if we have the correct number of genres\n",
        "\n",
        "print(f\"Number of images : {df.shape[0]}\")\n",
        "\n",
        "'''----------------------------------\n",
        "Train, test, val split\n",
        "----------------------------------'''\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
        "assert type(df_train) == type(pd.DataFrame()) # Check if we created dataframes\n",
        "assert type(df_test) == type(pd.DataFrame())\n",
        "assert type(df_val) == type(pd.DataFrame())\n",
        "\n",
        "print(f\"Number of images in train set : {df_train.shape[0]}\")\n",
        "print(f\"Number of images in val set : {df_val.shape[0]}\")\n",
        "print(f\"Number of images in test set : {df_test.shape[0]}\")\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Setup outputs columns\n",
        "----------------------------------'''\n",
        "assert len(list(df.columns[1:])) == 2 # Check if we have two outputs columns\n",
        "columns=list(df.columns[1:])\n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Train ImageDataGenerator\n",
        "----------------------------------'''\n",
        "train_datagen = ImageDataGenerator( # This generator is only used to train data because it has data augmentation and we do not want to augment data from the test or val set\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=(0.95, 0.95),\n",
        "    horizontal_flip=True,\n",
        "    dtype=tf.float32\n",
        "    )\n",
        "\n",
        "assert type(train_datagen) == type(ImageDataGenerator())\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=df_train, # Dataset used to get the path (column filename) and the linked outputs\n",
        "    directory=DIRECTORY + IMAGES, # Path to the images\n",
        "    x_col=\"file_name\", # Column with the name of the images that the generator will get from the directory\n",
        "    y_col=columns, # Columns with the output of the images that the generator will get from the csv\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=None,\n",
        "    shuffle=True,\n",
        "    class_mode=\"raw\", # numpy array of values in y_col columns\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Resize the images to the input shape of the model\n",
        "    data_format='channels_last'\n",
        "    ) \n",
        "\n",
        "\n",
        "'''----------------------------------\n",
        "Test and Val ImageDataGenerator\n",
        "----------------------------------'''\n",
        "test_val_datagen = ImageDataGenerator(  # We use a new generator without data augmentation\n",
        "    rescale=1./255\n",
        "    )\n",
        "\n",
        "val_generator = test_val_datagen.flow_from_dataframe(\n",
        "    dataframe=df_val, \n",
        "    directory=DIRECTORY + IMAGES,\n",
        "    x_col=\"file_name\",\n",
        "    y_col=columns,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=None,\n",
        "    shuffle=False,\n",
        "    class_mode=\"raw\", \n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    data_format='channels_last'\n",
        "    )\n",
        "\n",
        "test_generator = test_val_datagen.flow_from_dataframe(\n",
        "    dataframe=df_test,\n",
        "    directory=DIRECTORY + IMAGES,\n",
        "    x_col=\"file_name\",\n",
        "    batch_size=1,\n",
        "    seed=None,\n",
        "    shuffle=False,\n",
        "    class_mode=None, # No targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict()\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    data_format='channels_last'\n",
        "    )\n",
        "\n",
        "\n",
        "assert type(test_val_datagen) == type(ImageDataGenerator())\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "aPV2fKUWPuu9",
        "outputId": "a1cfda88-9673-4eee-f97b-b77ccc1ccb1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''----------------------------------\n",
        "Create model (Functional API)\n",
        "----------------------------------'''\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Model tout pourri pour l'instant, transfert learning ensuite\n",
        "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3), dtype=tf.float32)\n",
        "x = Conv2D(32, (3, 3), padding = 'same')(inputs)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(32, (3, 3))(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Conv2D(64, (3, 3), padding = 'same')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(64, (3, 3))(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(512)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "movement_output = Dense(NUM_MOVEMENT, activation = 'softmax')(x) # First output for movement\n",
        "genre_output = Dense(NUM_GENRE, activation = 'softmax')(x) # Second outptu for genre\n",
        "\n",
        "model = Model(inputs, [movement_output,genre_output]) # Create the model with 2 outputs\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "TpsswJ_CfbTv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''----------------------------------\n",
        "Compile model\n",
        "----------------------------------'''\n",
        "model.compile(\n",
        "    optimizer='adamax', \n",
        "    loss={'dense_1': keras.losses.CategoricalCrossentropy(), \n",
        "          'dense_2': keras.losses.CategoricalCrossentropy()},\n",
        "    metrics={\n",
        "        \"dense_1\": [\n",
        "            keras.metrics.Accuracy(),\n",
        "            keras.metrics.CategoricalAccuracy(),\n",
        "            ],\n",
        "        \"dense_2\": [\n",
        "            keras.metrics.Accuracy(),\n",
        "            keras.metrics.CategoricalAccuracy(),\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "# We have 2 losses one for each output"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''----------------------------------\n",
        "Setup callbacks and tensorboard\n",
        "----------------------------------'''\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=11, mode='min', restore_best_weights=True)\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=5, min_lr=1e-6)\n",
        "\n",
        "%load_ext tensorboard\n",
        "log_dir = f\"logs/{USER}/{MODEL_NAME}\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tsboard = TensorBoard(log_dir=log_dir)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''----------------------------------\n",
        "Fit model\n",
        "----------------------------------'''\n",
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size # see steps_per_epoch comment below\n",
        "STEP_SIZE_VAL = val_generator.n//val_generator.batch_size\n",
        "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
        "\n",
        "\n",
        "def generator_wrapper(generator):\n",
        "    for batch_x,batch_y in generator:\n",
        "        yield (batch_x,[batch_y[:,i] for i in range(2)])\n",
        "\n",
        "# The model does not see every image at each epoch !\n",
        "\n",
        "history = model.fit(\n",
        "    generator_wrapper(train_generator),\n",
        "    steps_per_epoch=STEP_SIZE_TRAIN, # No 'batch_size' parameter for data coming from a generator or tf.Dataset, common choice is to use num_samples // batch_size\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=generator_wrapper(val_generator), # We use a generator as the validation_data\n",
        "    validation_steps=STEP_SIZE_VAL,\n",
        "    callbacks=[es, rlrp, tsboard],\n",
        "    verbose=1\n",
        "    )\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''----------------------------------\n",
        "Evaluate\n",
        "----------------------------------'''\n",
        "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
        "pred = model.evaluate(\n",
        "    test_generator,\n",
        "    steps=STEP_SIZE_TEST,\n",
        "    verbose=1\n",
        "    )\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''----------------------------------\n",
        "Predict\n",
        "----------------------------------'''\n",
        "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
        "pred = model.predict(\n",
        "    test_generator,\n",
        "    steps=STEP_SIZE_TEST,\n",
        "    verbose=1\n",
        "    )\n"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}