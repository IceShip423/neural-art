{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b00ceb",
   "metadata": {},
   "source": [
    "# Image classification with 'image_dataset_from_directory' method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e662c",
   "metadata": {},
   "source": [
    "# Creating train/val/test datasets using split-folders package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b57d6b",
   "metadata": {},
   "source": [
    "The split-folders package allow us to create 3 folders of train/val/test images\n",
    "\n",
    "The input folder should have the following format (Gregoire's function output):\n",
    "\n",
    "input/\n",
    "\n",
    "    class1/\n",
    "        img1.jpg\n",
    "        img2.jpg\n",
    "        ...\n",
    "    class2/\n",
    "        imgWhatever.jpg\n",
    "        ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "And we get this\n",
    "\n",
    "output/\n",
    "\n",
    "    train/\n",
    "        class1/\n",
    "            img1.jpg\n",
    "            ...\n",
    "        class2/\n",
    "            imga.jpg\n",
    "            ...\n",
    "            \n",
    "    val/\n",
    "    \n",
    "        class1/\n",
    "        \n",
    "            img2.jpg\n",
    "            ...\n",
    "        class2/\n",
    "            imgb.jpg\n",
    "            ...\n",
    "    test/\n",
    "        class1/\n",
    "            img3.jpg\n",
    "            ...\n",
    "        class2/\n",
    "            imgc.jpg\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a26228",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "assert TRAIN_RATIO + VAL_RATIO + TEST_RATIO == 1\n",
    "assert TRAIN_RATIO != 0\n",
    "assert VAL_RATIO != 0\n",
    "assert TEST_RATIO != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb05186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 11520 files [00:03, 2940.17 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "!rm -rf \"/home/jupyter/wikiart/train_val_test_True_1440\"\n",
    "splitfolders.ratio(\"/home/jupyter/wikiart/wikiart-movement-genre_True-class_8-merge_mov-1-n_1440_max\", \n",
    "                   output=\"/home/jupyter/wikiart/train_val_test_True_1440\",\n",
    "                   seed=1337, ratio=(TRAIN_RATIO, VAL_RATIO, TEST_RATIO), \n",
    "                   group_prefix=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddacfee",
   "metadata": {},
   "source": [
    "# Deep learning workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ab823",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8523d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, applications\n",
    "\n",
    "# Note : we are using TensorFlow Core v2.5.0, in TensorFlow Core v2.6.0 all the data augmentaation layers are part of tf.keras.layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e608a7",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0384823",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'pablo'\n",
    "MODEL = 'VGG16'\n",
    "\n",
    "MAIN_PATH = '/home/jupyter/' \n",
    "DATASETS_FOLDER = 'wikiart/train_val_test_True_1440/'\n",
    "\n",
    "TRAIN_DIR = MAIN_PATH + DATASETS_FOLDER + 'train'\n",
    "VAL_DIR = MAIN_PATH + DATASETS_FOLDER + 'val'\n",
    "TEST_DIR = MAIN_PATH + DATASETS_FOLDER + 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8c2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1000\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "NUM_CLASSES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ca933",
   "metadata": {},
   "source": [
    "## Datasets setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0803b",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23eb56d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9216 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=TRAIN_DIR,\n",
    "    labels='inferred',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "assert len(train_ds.class_names) == NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c548756",
   "metadata": {},
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71cb7867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1152 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=VAL_DIR,\n",
    "    labels='inferred',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "assert len(val_ds.class_names) == NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745365dc",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc42523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1152 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=TEST_DIR,\n",
    "    labels='inferred',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "assert len(test_ds.class_names) == NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "282c4dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11520"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_images_count = (int(len(list(train_ds)))+int(len(list(val_ds)))+int(len(list(test_ds))))*BATCH_SIZE\n",
    "# total_images_count = 33011 + 4123 + 4134\n",
    "total_images_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18d48b",
   "metadata": {},
   "source": [
    "## Dataset optimization for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592f02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(int(total_images_count/10)).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c9ba7",
   "metadata": {},
   "source": [
    "## Model : transfer learning with VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8e7f4",
   "metadata": {},
   "source": [
    "### VGG16 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "391ba8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_model = applications.VGG16(\n",
    "    include_top=False, \n",
    "    weights='imagenet', \n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), \n",
    "    classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b614361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f215dbbd810>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215c78c410>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215c781650>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f215c7543d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215e219750>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215a857f10>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f215e219350>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215a858590>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6b94710>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6b99e90>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f2158656950>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6ba2a10>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6ba9050>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6ba2d10>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f21d6bb3b50>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21586d8c10>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6bbdb90>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6bb0710>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f21d6b49410>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b1e2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "325f870d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6bb0710>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f21d6b49410>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_model.layers[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e7ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layer_model.layers[-2:]:\n",
    "    layer.trainable = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2a5efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_layer_count = 0\n",
    "\n",
    "for i in range(len(layer_model.layers)):\n",
    "    if layer_model.layers[i].trainable:\n",
    "        trainable_layer_count += 1\n",
    "        \n",
    "trainable_layer_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39617a49",
   "metadata": {},
   "source": [
    "### Data augmentation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18f519f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_layers = models.Sequential([\n",
    "    RandomFlip(\"horizontal\", input_shape=(224, 224,3)),\n",
    "    RandomRotation(0.3),\n",
    "    RandomZoom(0.3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf8aba",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f099cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() # Clear the layers name (in case you run multiple time the cell)\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = data_augmentation_layers(inputs) # Are not applied to validation and test dataset (made inactive, tensorflow handle it)\n",
    "x = applications.vgg16.preprocess_input(x) # Does the rescaling\n",
    "x = layer_model(x) \n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.5)(x) \n",
    "\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='classification_layer')(x)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad241d",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490b248",
   "metadata": {},
   "source": [
    "#### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf277ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=20, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0add62",
   "metadata": {},
   "source": [
    "#### ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "725cd09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=3, min_lr=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278c5ff",
   "metadata": {},
   "source": [
    "#### ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7276eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = f\"{MAIN_PATH}logs/{USERNAME}/{MODEL}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"-unfreeze_{trainable_layer_count}\"\n",
    "mcp = ModelCheckpoint(\n",
    "    filepath=checkpoint_dir,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_freq=10,\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35b28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# !rm -rf ./logs/ResNet50\n",
    "recorded_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "log_dir = f\"{MAIN_PATH}logs/{USERNAME}/{MODEL}/\" + \\\n",
    "    recorded_time + \\\n",
    "    f\"-images_{total_images_count}\" + \\\n",
    "    f\"-unfreeze_{trainable_layer_count}\" + \\\n",
    "    f\"-batch_{BATCH_SIZE}\"\n",
    "tsboard = TensorBoard(log_dir=log_dir) # voir https public"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab83c87",
   "metadata": {},
   "source": [
    "### Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0382949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adamax(learning_rate=0.001), \n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477329c3",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f80c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "72/72 [==============================] - 86s 616ms/step - loss: 4.3145 - accuracy: 0.3708 - val_loss: 1.9313 - val_accuracy: 0.5347\n",
      "Epoch 2/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 2.3179 - accuracy: 0.4596 - val_loss: 1.6383 - val_accuracy: 0.5391\n",
      "Epoch 3/1000\n",
      "72/72 [==============================] - 43s 592ms/step - loss: 1.8028 - accuracy: 0.4869 - val_loss: 1.3055 - val_accuracy: 0.5582\n",
      "Epoch 4/1000\n",
      "72/72 [==============================] - 43s 596ms/step - loss: 1.5749 - accuracy: 0.4976 - val_loss: 1.2923 - val_accuracy: 0.5747\n",
      "Epoch 5/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.5137 - accuracy: 0.4976 - val_loss: 1.2581 - val_accuracy: 0.5530\n",
      "Epoch 6/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.5201 - accuracy: 0.5005 - val_loss: 1.2272 - val_accuracy: 0.5521\n",
      "Epoch 7/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.4973 - accuracy: 0.4916 - val_loss: 1.2226 - val_accuracy: 0.5608\n",
      "Epoch 8/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.3835 - accuracy: 0.5146 - val_loss: 1.1409 - val_accuracy: 0.5781\n",
      "Epoch 9/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.3000 - accuracy: 0.5308 - val_loss: 1.1093 - val_accuracy: 0.5903\n",
      "Epoch 10/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2948 - accuracy: 0.5294 - val_loss: 1.1020 - val_accuracy: 0.5911\n",
      "Epoch 11/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2819 - accuracy: 0.5328 - val_loss: 1.1210 - val_accuracy: 0.5877\n",
      "Epoch 12/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.2804 - accuracy: 0.5329 - val_loss: 1.1332 - val_accuracy: 0.5703\n",
      "Epoch 13/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.2907 - accuracy: 0.5301 - val_loss: 1.0814 - val_accuracy: 0.5998\n",
      "Epoch 14/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.2993 - accuracy: 0.5244 - val_loss: 1.1229 - val_accuracy: 0.5738\n",
      "Epoch 15/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2840 - accuracy: 0.5311 - val_loss: 1.1167 - val_accuracy: 0.5790\n",
      "Epoch 16/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.2958 - accuracy: 0.5286 - val_loss: 1.0670 - val_accuracy: 0.5929\n",
      "Epoch 17/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2263 - accuracy: 0.5393 - val_loss: 1.0296 - val_accuracy: 0.6111\n",
      "Epoch 18/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.2240 - accuracy: 0.5492 - val_loss: 1.0436 - val_accuracy: 0.6042\n",
      "Epoch 19/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.2167 - accuracy: 0.5481 - val_loss: 1.0475 - val_accuracy: 0.5964\n",
      "Epoch 20/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2146 - accuracy: 0.5485 - val_loss: 1.0570 - val_accuracy: 0.5946\n",
      "Epoch 21/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.2091 - accuracy: 0.5495 - val_loss: 1.0413 - val_accuracy: 0.6007\n",
      "Epoch 22/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1984 - accuracy: 0.5524 - val_loss: 1.0419 - val_accuracy: 0.6024\n",
      "Epoch 23/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1886 - accuracy: 0.5581 - val_loss: 1.0399 - val_accuracy: 0.5998\n",
      "Epoch 24/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1807 - accuracy: 0.5587 - val_loss: 1.0337 - val_accuracy: 0.5990\n",
      "Epoch 25/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.1860 - accuracy: 0.5637 - val_loss: 1.0315 - val_accuracy: 0.6085\n",
      "Epoch 26/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1866 - accuracy: 0.5627 - val_loss: 1.0394 - val_accuracy: 0.5946\n",
      "Epoch 27/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1837 - accuracy: 0.5574 - val_loss: 1.0360 - val_accuracy: 0.5981\n",
      "Epoch 28/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1738 - accuracy: 0.5636 - val_loss: 1.0342 - val_accuracy: 0.6042\n",
      "Epoch 29/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1835 - accuracy: 0.5594 - val_loss: 1.0332 - val_accuracy: 0.6068\n",
      "Epoch 30/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1689 - accuracy: 0.5716 - val_loss: 1.0325 - val_accuracy: 0.6076\n",
      "Epoch 31/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1867 - accuracy: 0.5543 - val_loss: 1.0331 - val_accuracy: 0.6085\n",
      "Epoch 32/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1577 - accuracy: 0.5615 - val_loss: 1.0343 - val_accuracy: 0.6050\n",
      "Epoch 33/1000\n",
      "72/72 [==============================] - 43s 596ms/step - loss: 1.1823 - accuracy: 0.5548 - val_loss: 1.0342 - val_accuracy: 0.6050\n",
      "Epoch 34/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1690 - accuracy: 0.5673 - val_loss: 1.0333 - val_accuracy: 0.6042\n",
      "Epoch 35/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1689 - accuracy: 0.5650 - val_loss: 1.0327 - val_accuracy: 0.6033\n",
      "Epoch 36/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1840 - accuracy: 0.5540 - val_loss: 1.0331 - val_accuracy: 0.6024\n",
      "Epoch 37/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1791 - accuracy: 0.5604 - val_loss: 1.0330 - val_accuracy: 0.6033\n",
      "INFO:tensorflow:Assets written to: /home/jupyter/models/pablo/VGG16/20210830-122006-images_11520-unfreeze_2-batch_128/assets\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=val_ds, \n",
    "    callbacks=[es, rlrp, tsboard], \n",
    "    use_multiprocessing=True)\n",
    "\n",
    "model.save(f\"{MAIN_PATH}models/{USERNAME}/{MODEL}/\" + \\\n",
    "    recorded_time + \\\n",
    "    f\"-images_{total_images_count}\" + \\\n",
    "    f\"-unfreeze_{trainable_layer_count}\" + \\\n",
    "    f\"-batch_{BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = history.epoch\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "fig.savefig(f\"/home/jupyter/figures/{USERNAME}/{MODEL}/{recorded_time}-images_{total_images_count}-unfreeze_{trainable_layer_count}-batch_{BATCH_SIZE}\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc015a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, callbacks=tsboard)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
