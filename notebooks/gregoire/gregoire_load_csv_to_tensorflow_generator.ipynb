{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AylCVVQvOkIB"
   },
   "source": [
    "# Multi-label image classification with ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_9Sg9B4zHmCm"
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Dropout, Input, Conv2D, Flatten, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Accuracy, CategoricalAccuracy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images : 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAIN_PATH = \"../raw_data/wikiart/\" # Path to the directory which contains CSVs and the folder 'dataset'\n",
    "IMAGES = \"dataset\"\n",
    "CSV_NAME = \"wikiart-movement-genre_True-class_3-merge_test1-n_100_max.csv\" # Nwikiart-movement-genre_True-class_3-merge_test1-n_1000_max.csvme of the CSV we want to use\n",
    "NUM_MOVEMENT = 3 # Number of movements to classify\n",
    "NUM_GENRE = 10 # Number of genres to classify\n",
    "IMG_HEIGHT = IMG_WIDTH = 224 # Model's inputs shapes\n",
    "\n",
    "USER = \"gregoire\" # Choose between 'common', 'pablo', 'quentin', 'gregoire', 'alex'\n",
    "MODEL_NAME = \"Custom v1\" # Set the name of the model \n",
    "\n",
    "'''----------------------------------\n",
    "Load the CSV\n",
    "----------------------------------'''\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "'''----------------------------------\n",
    "Load the CSV\n",
    "----------------------------------'''\n",
    "raw_df = pd.read_csv(MAIN_PATH + CSV_NAME)\n",
    "df = raw_df.iloc[:, 0:3]\n",
    "assert type(df) == type(pd.DataFrame()) # Check if we created a dataframe\n",
    "assert df.iloc[:, 1].nunique() ==  NUM_MOVEMENT # Check if we have the correct number of movements\n",
    "assert df.iloc[:, 2].nunique() ==  NUM_GENRE # Check if we have the correct number of genres\n",
    "\n",
    "print(f\"Number of images : {df.shape[0]}\")\n",
    "\n",
    "'''----------------------------------\n",
    "OHE\n",
    "----------------------------------'''\n",
    "\n",
    "df_genre_ohe = pd.get_dummies(df['genre'])\n",
    "df_mov_ohe = pd.get_dummies(df['movement'])\n",
    "\n",
    "df = pd.concat([df,df_genre_ohe, df_mov_ohe], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "aPV2fKUWPuu9",
    "outputId": "a1cfda88-9673-4eee-f97b-b77ccc1ccb1f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train set : 192\n",
      "Number of images in val set : 48\n",
      "Number of images in test set : 60\n",
      "\n",
      "\n",
      "Found 192 validated image filenames.\n",
      "Found 48 validated image filenames.\n",
      "Found 60 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "'''----------------------------------\n",
    "Train, test, val split\n",
    "----------------------------------'''\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
    "# assert type(df_train) == type(pd.DataFrame()) # Check if we created dataframes\n",
    "# assert type(df_test) == type(pd.DataFrame())\n",
    "# assert type(df_val) == type(pd.DataFrame())\n",
    "\n",
    "print(f\"Number of images in train set : {df_train.shape[0]}\")\n",
    "print(f\"Number of images in val set : {df_val.shape[0]}\")\n",
    "print(f\"Number of images in test set : {df_test.shape[0]}\")\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "'''----------------------------------\n",
    "Setup outputs columns\n",
    "----------------------------------'''\n",
    "#assert len(list(df.columns[1:])) == 2 # Check if we have two outputs columns\n",
    "#columns=list(df.columns[1:])\n",
    "\n",
    "columns_genre = list(df_genre_ohe.columns)\n",
    "columns_mov = list(df_mov_ohe.columns)\n",
    "columns = columns_mov + columns_genre \n",
    "\n",
    "'''----------------------------------\n",
    "Train ImageDataGenerator\n",
    "----------------------------------'''\n",
    "train_datagen = ImageDataGenerator( # This generator is only used to train data because it has data augmentation and we do not want to augment data from the test or val set\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    zoom_range=(0.95, 0.95),\n",
    "    horizontal_flip=True,\n",
    "    dtype=tf.float32\n",
    "    )\n",
    "\n",
    "#assert type(train_datagen) == type(ImageDataGenerator())\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train, # Dataset used to get the path (column filename) and the linked outputs\n",
    "    directory=MAIN_PATH + IMAGES, # Path to the images\n",
    "    x_col=\"file_name\", # Column with the name of the images that the generator will get from the directory\n",
    "    y_col=columns, # Columns with the output of the images that the generator will get from the csv\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=None,\n",
    "    shuffle=True,\n",
    "    class_mode=\"raw\", # numpy array of values in y_col columns\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Resize the images to the input shape of the model\n",
    "    data_format='channels_last'\n",
    "    ) \n",
    "\n",
    "\n",
    "'''----------------------------------\n",
    "Test and Val ImageDataGenerator\n",
    "----------------------------------'''\n",
    "test_val_datagen = ImageDataGenerator(  # We use a new generator without data augmentation\n",
    "    rescale=1./255\n",
    "    )\n",
    "\n",
    "val_generator = test_val_datagen.flow_from_dataframe(\n",
    "    dataframe=df_val, \n",
    "    directory=MAIN_PATH + IMAGES,\n",
    "    x_col=\"file_name\",\n",
    "    y_col=columns,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=None,\n",
    "    shuffle=False,\n",
    "    class_mode=\"raw\", \n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    data_format='channels_last'\n",
    "    )\n",
    "\n",
    "test_generator = test_val_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    directory=MAIN_PATH + IMAGES,\n",
    "    x_col=\"file_name\",\n",
    "    batch_size=1,\n",
    "    seed=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None, # No targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict()\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    data_format='channels_last'\n",
    "    )\n",
    "\n",
    "\n",
    "#assert type(test_val_datagen) == type(ImageDataGenerator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Create model (Functional API)\n",
    "----------------------------------'''\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Model tout pourri pour l'instant, transfert learning ensuite\n",
    "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3), dtype=tf.float32)\n",
    "x = Conv2D(16, 3, padding='same', activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = Dropout(rate=0.3)(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.4)(x)\n",
    "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32)(x)\n",
    "\n",
    "movement_output = Dense(NUM_MOVEMENT, activation = 'softmax', name='movement_output')(x) # First output for movement\n",
    "genre_output = Dense(NUM_GENRE, activation = 'softmax', name='genre_output')(x) # Second outptu for genre\n",
    "\n",
    "model = Model(inputs, [movement_output,genre_output]) # Create the model with 2 outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 3), (None, 10)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Compile model\n",
    "----------------------------------'''\n",
    "model.compile(\n",
    "    optimizer='adamax', \n",
    "    loss={'movement_output': CategoricalCrossentropy(), \n",
    "          'genre_output': CategoricalCrossentropy()},\n",
    "    metrics={\n",
    "        \"movement_output\": [\n",
    "            Accuracy(),\n",
    "            ],\n",
    "        \"genre_output\": [\n",
    "            Accuracy(),\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "# We have 2 losses one for each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-25 18:07:39.573777: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-08-25 18:07:39.573798: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-08-25 18:07:39.574158: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "'''----------------------------------\n",
    "Setup callbacks and tensorboard\n",
    "----------------------------------'''\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=11, mode='min', restore_best_weights=True)\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=5, min_lr=1e-6)\n",
    "\n",
    "%load_ext tensorboard\n",
    "log_dir = f\"logs/{USER}/{MODEL_NAME}\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tsboard = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 224, 224, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_generator))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_generator))[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TpsswJ_CfbTv"
   },
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Fit model\n",
    "----------------------------------'''\n",
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size # see steps_per_epoch comment below\n",
    "STEP_SIZE_VAL = val_generator.n//val_generator.batch_size\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
    "\n",
    "\n",
    "\n",
    "# We now have the model that has 2 output layers but our train_generator and valid_generator outputs a single array for the target labels, to handle this we need to write a python generator\n",
    "# function that takes train_generator or valid_generator as input and yields a tuple containing the images and a list containing 2(No. of outputs) arrays for the target labels.\n",
    "def generator_wrapper(generator):\n",
    "    for batch_x,batch_y in generator:\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "        yield batch_x, [batch_y[:,:3], batch_y[:,3:]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-25 18:07:51.199617: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/6 [====>.........................] - ETA: 14s - loss: 3.4013 - movement_output_loss: 1.1174 - genre_output_loss: 2.2838 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-25 18:07:54.299854: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-08-25 18:07:54.299883: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/6 [=========>....................] - ETA: 12s - loss: 5.9993 - movement_output_loss: 1.9321 - genre_output_loss: 4.0672 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-25 18:07:57.277063: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-08-25 18:07:57.280087: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-08-25 18:07:57.285016: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57\n",
      "\n",
      "2021-08-25 18:07:57.286603: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57/MacBook-Pro-de-Camille-2.local.trace.json.gz\n",
      "2021-08-25 18:07:57.297168: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57\n",
      "\n",
      "2021-08-25 18:07:57.297657: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57/MacBook-Pro-de-Camille-2.local.memory_profile.json.gz\n",
      "2021-08-25 18:07:57.300343: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57\n",
      "Dumped tool data for xplane.pb to logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57/MacBook-Pro-de-Camille-2.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57/MacBook-Pro-de-Camille-2.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57/MacBook-Pro-de-Camille-2.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57/MacBook-Pro-de-Camille-2.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/gregoire/Custom v120210825-180739/train/plugins/profile/2021_08_25_18_07_57/MacBook-Pro-de-Camille-2.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 21s 4s/step - loss: 4.7853 - movement_output_loss: 1.6001 - genre_output_loss: 3.1852 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00 - val_loss: 3.3824 - val_movement_output_loss: 1.1018 - val_genre_output_loss: 2.2806 - val_movement_output_accuracy: 0.0000e+00 - val_genre_output_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 3.3421 - movement_output_loss: 1.0831 - genre_output_loss: 2.2590 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00 - val_loss: 3.3336 - val_movement_output_loss: 1.0951 - val_genre_output_loss: 2.2385 - val_movement_output_accuracy: 0.0000e+00 - val_genre_output_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.0908 - movement_output_loss: 1.0600 - genre_output_loss: 2.0308 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00 - val_loss: 3.0895 - val_movement_output_loss: 1.0841 - val_genre_output_loss: 2.0054 - val_movement_output_accuracy: 0.0000e+00 - val_genre_output_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 15s 3s/step - loss: 3.0458 - movement_output_loss: 1.0835 - genre_output_loss: 1.9623 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00 - val_loss: 2.9747 - val_movement_output_loss: 1.0795 - val_genre_output_loss: 1.8952 - val_movement_output_accuracy: 0.0000e+00 - val_genre_output_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 14s 2s/step - loss: 3.0475 - movement_output_loss: 1.0668 - genre_output_loss: 1.9807 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00 - val_loss: 3.0058 - val_movement_output_loss: 1.0805 - val_genre_output_loss: 1.9253 - val_movement_output_accuracy: 0.0000e+00 - val_genre_output_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 13s 2s/step - loss: 2.9383 - movement_output_loss: 1.0571 - genre_output_loss: 1.8811 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00 - val_loss: 2.9792 - val_movement_output_loss: 1.0548 - val_genre_output_loss: 1.9244 - val_movement_output_accuracy: 0.0000e+00 - val_genre_output_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.9155 - movement_output_loss: 0.9955 - genre_output_loss: 1.9200 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00 - val_loss: 2.9335 - val_movement_output_loss: 1.0505 - val_genre_output_loss: 1.8830 - val_movement_output_accuracy: 0.0000e+00 - val_genre_output_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 14s 2s/step - loss: 2.8554 - movement_output_loss: 1.0451 - genre_output_loss: 1.8103 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00 - val_loss: 2.9062 - val_movement_output_loss: 1.0410 - val_genre_output_loss: 1.8652 - val_movement_output_accuracy: 0.0000e+00 - val_genre_output_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "4/6 [===================>..........] - ETA: 4s - loss: 2.9218 - movement_output_loss: 1.0129 - genre_output_loss: 1.9089 - movement_output_accuracy: 0.0000e+00 - genre_output_accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/prx4ncw52tgg975t9s36p5r80000gp/T/ipykernel_15151/2865693679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The model does not see every image at each epoch !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgenerator_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# No 'batch_size' parameter for data coming from a generator or tf.Dataset, common choice is to use num_samples // batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/neuralart/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/neuralart/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/neuralart/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/neuralart/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/neuralart/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/neuralart/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/neuralart/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The model does not see every image at each epoch !\n",
    "\n",
    "history = model.fit(\n",
    "    generator_wrapper(train_generator),\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN, # No 'batch_size' parameter for data coming from a generator or tf.Dataset, common choice is to use num_samples // batch_size\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=generator_wrapper(val_generator), # We use a generator as the validation_data\n",
    "    validation_steps=STEP_SIZE_VAL,\n",
    "    callbacks=[es, rlrp, tsboard],\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Evaluate\n",
    "----------------------------------'''\n",
    "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
    "pred = model.evaluate(\n",
    "    test_generator,\n",
    "    steps=STEP_SIZE_TEST,\n",
    "    verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''----------------------------------\n",
    "Predict\n",
    "----------------------------------'''\n",
    "test_generator.reset() # Need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.\n",
    "pred = model.predict(\n",
    "    test_generator,\n",
    "    steps=STEP_SIZE_TEST,\n",
    "    verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pablo-load_csv_to_tensorflow_generator",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
